{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a90781a",
   "metadata": {},
   "source": [
    "# Testing Sentence Embeddings\n",
    "In this notebook we illustrate that running PCA on a collection of sentence embeddings prior to feeding the embeddings into a classifier produces better results than no pca or pca performed during generating the sentence level embeddings. We use data from the\n",
    "Semantic Text Similarity Dataset Hub https://github.com/brmson/dataset-sts and its Semantic Similarity Task.\n",
    "We will use the GoogleNews embeddings, but these results should be generalizable.\n",
    "\n",
    "Summary Results, using Mean Square Error:\n",
    "* PCA used to generate a sentence embedding, Test results: loss 1.189 acc: 1.189\n",
    "* No PCA, Test results: loss 0.703 acc: 0.703\n",
    "* PCA only on the collection sentence embeddings, Test results: loss 0.678 acc: 0.678\n",
    "\n",
    "Note: The data categorizes gold standard as a score between 0 and 5 for each pair of\n",
    "sentences, with the following interpretation:\n",
    "* (5) The two sentences are completely equivalent, as they mean the same\n",
    "    thing.  \n",
    "* (4) The two sentences are mostly equivalent, but some unimportant\n",
    "    details differ.\n",
    "* (3) The two sentences are roughly equivalent, but some important\n",
    "    information differs/missing.\n",
    "* (2) The two sentences are not equivalent, but share some details.\n",
    "* (1) The two sentences are not equivalent, but are on the same topic.\n",
    "* (0) The two sentences are on different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379da54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "from random import sample\n",
    "from typing import Optional, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk import word_tokenize\n",
    "import tensorflow as tf\n",
    "\n",
    "currentdir = Path.cwd()\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "from mlyoucanuse.embeddings import get_embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c117dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2f0043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['espresso_martinis', 'wherefore', 'HARD', 'courtly_manners', \"Hawai'ian\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding = get_embeddings_index('GoogleNews', parent_dir=parentdir, embedding_dimensions=300)\n",
    "sample(list(token_embedding.keys()), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa3f21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['../data/dataset-sts/data/sts/semeval-sts/2012/MSRpar.train.tsv',\n",
       "  '../data/dataset-sts/data/sts/semeval-sts/2012/SMTeuroparl.train.tsv',\n",
       "  '../data/dataset-sts/data/sts/semeval-sts/all/2015.val.tsv',\n",
       "  '../data/dataset-sts/data/sts/semeval-sts/all/2015.train.tsv'],\n",
       " ['../data/dataset-sts/data/sts/semeval-sts/2012/MSRpar.test.tsv',\n",
       "  '../data/dataset-sts/data/sts/semeval-sts/2012/SMTeuroparl.test.tsv',\n",
       "  '../data/dataset-sts/data/sts/semeval-sts/2012/OnWN.test.tsv',\n",
       "  '../data/dataset-sts/data/sts/semeval-sts/2012/SMTnews.test.tsv',\n",
       "  '../data/dataset-sts/data/sts/semeval-sts/all/2015.test.tsv'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files = glob(\"../data/dataset-sts/data/sts/semeval-sts/2012/*.train.tsv\")\n",
    "test_files = glob(\"../data/dataset-sts/data/sts/semeval-sts/2012/*.test.tsv\")\n",
    "\n",
    "train_files.append( '../data/dataset-sts/data/sts/semeval-sts/all/2015.val.tsv')\n",
    "train_files.append( '../data/dataset-sts/data/sts/semeval-sts/all/2015.train.tsv')\n",
    "test_files.append( '../data/dataset-sts/data/sts/semeval-sts/all/2015.test.tsv')\n",
    "\n",
    "train_files, test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54904256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  7.46it/s]\n",
      "/Users/todd/opt/anaconda3/envs/mlycu3.8/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "  1%|▏         | 254/18090 [00:00<00:07, 2535.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocab: 18,090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18090/18090 [00:07<00:00, 2365.30it/s]\n"
     ]
    }
   ],
   "source": [
    "all_docs = []\n",
    "for file in tqdm(train_files + test_files):\n",
    "    df = pd.read_csv(file, sep='\\t', names=['score', 'sent1', 'sent2'], header=None)\n",
    "    df.dropna(inplace=True)\n",
    "    for idx, row in df.iterrows():\n",
    "        all_docs.append(row['sent1'])\n",
    "        all_docs.append(row['sent2'])\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize) \n",
    "vectorizer.fit(all_docs)\n",
    "print(f\"size of vocab: {len(vectorizer.vocabulary_):,}\")\n",
    "word_idf = {key: vectorizer.idf_[idx] \n",
    "            for key,idx in tqdm(vectorizer.vocabulary_.items(), total=len(vectorizer.idf_))}\n",
    "del vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "febeb36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values are needed while generating sentence embeddings\n",
    "min_idf = np.min(np.array(list(word_idf.values())))\n",
    "max_idf = np.max(np.array(list(word_idf.values())))\n",
    "mean_idf = np.mean(np.array(list(word_idf.values())))\n",
    "\n",
    "def rescale_idf(val):\n",
    "    return (val - min_idf) / (max_idf - min_idf)\n",
    "\n",
    "def compute_pc(X, npc=1):\n",
    "    \"\"\"\n",
    "    Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: component_[i,:] is the i-th pc\n",
    "    \n",
    "    # This has been adapted from the SIF paper code: https://openreview.net/pdf?id=SyK00v5xx\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "    svd.fit(X)\n",
    "    return svd.components_\n",
    "\n",
    "def remove_pc(X, npc=1):\n",
    "    \"\"\"\n",
    "    Remove the projection on the principal components\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: XX[i, :] is the data point after removing its projection\n",
    "    \n",
    "    # This has been adapted from the SIF paper code: https://openreview.net/pdf?id=SyK00v5xx\n",
    "    \"\"\"\n",
    "    pc = compute_pc(X, npc)\n",
    "    if npc==1:\n",
    "        XX = X - X.dot(pc.transpose()) * pc\n",
    "    else:\n",
    "        XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "    return XX\n",
    "\n",
    "def get_sent_embeddings(text:str,\n",
    "                        word_idf_map:Optional[Dict[str,float]]=None,\n",
    "                       use_pca=True):\n",
    "    \"\"\"\n",
    "    Provides the weighted average of a sentence's word vectors with the principle component removed.\n",
    "    \n",
    "    Expectations:\n",
    "    Word can only appear once in a sentence, multiple occurrences are collapsed.\n",
    "    Must have 2 or more embeddings, otherwise Principle Component cannot be found and removed.\n",
    "    \n",
    "    \"\"\"\n",
    "    if word_idf_map:\n",
    "        word_idf = word_idf_map\n",
    "    # else: load pickle file\n",
    "    tokens = word_tokenize(text)\n",
    "    embed_map ={\n",
    "        tok.lower(): \n",
    "        (rescale_idf(word_idf.get(tok.lower(), min_idf)), token_embedding.get(tok,0)  )\n",
    "        for tok in tokens\n",
    "        if not np.all( (token_embedding.get(tok,0) ==0)) # skip empty embeddings\n",
    "    }\n",
    "    words = embed_map.keys()\n",
    "    weights_embedds = embed_map.values()\n",
    "    if len(weights_embedds) < 2: # we can't create a sentence embedding for just one word\n",
    "        return np.zeros(300)    \n",
    "    weights, embedds = zip(*weights_embedds)\n",
    "    if sum(weights) == 0:\n",
    "        return np.zeros(300)    \n",
    "    embedds = np.array(embedds)    \n",
    "    if use_pca:\n",
    "        embedds = remove_pc(embedds)    \n",
    "    scale_factor = 1 / sum(weights) \n",
    "    scaled_vals = np.array([tmp * scale_factor for tmp in weights])\n",
    "    # apply our weighted terms to the adjusted embeddings\n",
    "    weighted_embeds = embedds * scaled_vals[:,None]\n",
    "    mean_wt_embed = np.sum(weighted_embeds, axis=0)\n",
    "    return mean_wt_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fd59b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy(pca_flag):\n",
    "    X =[]\n",
    "    y = []\n",
    "    for file in train_files + test_files:\n",
    "        df = pd.read_csv(file, sep='\\t', names=['score', 'sent1', 'sent2'], header=None)\n",
    "        df.dropna(inplace=True)\n",
    "        for idx, row in df.iterrows():\n",
    "            X.append(np.concatenate([\n",
    "                get_sent_embeddings(row['sent1'], word_idf, use_pca=pca_flag),\n",
    "                 get_sent_embeddings(row['sent2'], word_idf, use_pca=pca_flag)]))\n",
    "            y.append(row['score'])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45d948d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 1s 3ms/step - loss: 0.0713 - mean_squared_error: 0.0713\n",
      "Using PCA\n",
      "Train results: loss 0.071 acc: 0.071\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 1.1252 - mean_squared_error: 1.1252\n",
      "Validation results: loss 1.125 acc: 1.125\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 1.1891 - mean_squared_error: 1.1891\n",
      "Test results: loss 1.189 acc: 1.189\n",
      "357/357 [==============================] - 1s 3ms/step - loss: 0.0489 - mean_squared_error: 0.0489\n",
      "Without PCA\n",
      "Train results: loss 0.049 acc: 0.049\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.7718 - mean_squared_error: 0.7718\n",
      "Validation results: loss 0.772 acc: 0.772\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.7027 - mean_squared_error: 0.7027\n",
      "Test results: loss 0.703 acc: 0.703\n"
     ]
    }
   ],
   "source": [
    "X, y = get_xy(pca_flag=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=.8, \n",
    "                                                    random_state=12,\n",
    "                                                   stratify=np.round(y))\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test,\n",
    "                                                              train_size=.5,\n",
    "                                                              random_state=12,\n",
    "                                                              stratify=np.round(y_test))\n",
    " \n",
    "## Build a simple model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(1024, input_shape=(600,), activation='relu'),\n",
    "    layers.Dense(512, activation='relu'), \n",
    "    layers.Dense(1, activation=\"linear\")\n",
    "])\n",
    "model.compile(loss='mean_squared_error', \n",
    "              optimizer='adam',\n",
    "              metrics=['mean_squared_error'])\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_validation, y_validation),\n",
    "                    verbose=0)\n",
    "## Evaluate and adjust as necessary\n",
    "res = model.evaluate(X_train, y_train)\n",
    "print('Using PCA')\n",
    "print(f\"Train results: loss {res[0]:.3f} acc: {res[1]:.3f}\")\n",
    "res =  model.evaluate(X_validation, y_validation)\n",
    "print(f\"Validation results: loss {res[0]:.3f} acc: {res[1]:.3f}\")\n",
    "res = model.evaluate(X_test, y_test)\n",
    "print(f\"Test results: loss {res[0]:.3f} acc: {res[1]:.3f}\")\n",
    "\n",
    "#######################\n",
    "\n",
    "X, y = get_xy(pca_flag=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=.8, \n",
    "                                                    random_state=12,\n",
    "                                                   stratify=np.round(y))\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test,\n",
    "                                                              train_size=.5,\n",
    "                                                              random_state=12,\n",
    "                                                              stratify=np.round(y_test))\n",
    " \n",
    "## Build a simple model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(1024, input_shape=(600,), activation='relu'),\n",
    "    layers.Dense(512, activation='relu'), \n",
    "    layers.Dense(1, activation=\"linear\")\n",
    "])\n",
    "model.compile(loss='mean_squared_error', \n",
    "              optimizer='adam',\n",
    "              metrics=['mean_squared_error'])\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_validation, y_validation),\n",
    "                    verbose=0)\n",
    "## Evaluate and adjust as necessary\n",
    "res = model.evaluate(X_train, y_train)\n",
    "print('Without PCA')\n",
    "print(f\"Train results: loss {res[0]:.3f} acc: {res[1]:.3f}\")\n",
    "res =  model.evaluate(X_validation, y_validation)\n",
    "print(f\"Validation results: loss {res[0]:.3f} acc: {res[1]:.3f}\")\n",
    "res = model.evaluate(X_test, y_test)\n",
    "print(f\"Test results: loss {res[0]:.3f} acc: {res[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2891eaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: (11396, 600), X_test size: (1425, 600), X_validation size: (1425, 600)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train size: {X_train.shape}, X_test size: {X_test.shape}, X_validation size: {X_validation.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14acd0cb",
   "metadata": {},
   "source": [
    "# Now let's only run PCA on the collection of sentences\n",
    "Note: this isn't very practical when creating and dealing with sentence embeddings in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9ce103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xone, xtwo = np.hsplit(X, 2)\n",
    "xcopy = np.vstack([xone, xtwo])\n",
    "xcopy = remove_pc(xcopy)\n",
    "x_one, x_two = np.vsplit(xcopy, 2)\n",
    "Xpca =  np.hstack([x_one, x_two])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "718e6f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 1s 3ms/step - loss: 0.0459 - mean_squared_error: 0.0459\n",
      "PCA only on the collection\n",
      "Train results: loss 0.046 acc: 0.046\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6707 - mean_squared_error: 0.6707\n",
      "Validation results: loss 0.671 acc: 0.671\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6778 - mean_squared_error: 0.6778\n",
      "Test results: loss 0.678 acc: 0.678\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xpca, y,\n",
    "                                                    train_size=.8, \n",
    "                                                    random_state=12,\n",
    "                                                   stratify=np.round(y))\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test,\n",
    "                                                              train_size=.5,\n",
    "                                                              random_state=12,\n",
    "                                                              stratify=np.round(y_test))\n",
    "\n",
    "## Build a simple model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(1024, input_shape=(600,), activation='relu'),\n",
    "    layers.Dense(512, activation='relu'), \n",
    "    layers.Dense(1, activation=\"linear\")\n",
    "])\n",
    "model.compile(loss='mean_squared_error', \n",
    "              optimizer='adam',\n",
    "              metrics=['mean_squared_error'])\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_validation, y_validation),\n",
    "                    verbose=0)\n",
    "## Evaluate and adjust as necessary\n",
    "res = model.evaluate(X_train, y_train)\n",
    "print('PCA only on the collection')\n",
    "print(f\"Train results: loss {res[0]:.3f} acc: {res[1]:.3f}\")\n",
    "res =  model.evaluate(X_validation, y_validation)\n",
    "print(f\"Validation results: loss {res[0]:.3f} acc: {res[1]:.3f}\")\n",
    "res = model.evaluate(X_test, y_test)\n",
    "print(f\"Test results: loss {res[0]:.3f} acc: {res[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489c733",
   "metadata": {},
   "source": [
    "# Summary results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2391d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PCA\n",
    "# Train results: loss 0.071 acc: 0.071\n",
    "# 45/45 [==============================] - 0s 3ms/step - loss: 1.1252 - mean_squared_error: 1.1252\n",
    "# Validation results: loss 1.125 acc: 1.125\n",
    "# 45/45 [==============================] - 0s 2ms/step - loss: 1.1891 - mean_squared_error: 1.1891\n",
    "# Test results: loss 1.189 acc: 1.189\n",
    "\n",
    "# Without PCA\n",
    "# Train results: loss 0.049 acc: 0.049\n",
    "# 45/45 [==============================] - 0s 3ms/step - loss: 0.7718 - mean_squared_error: 0.7718\n",
    "# Validation results: loss 0.772 acc: 0.772\n",
    "# 45/45 [==============================] - 0s 2ms/step - loss: 0.7027 - mean_squared_error: 0.7027\n",
    "# Test results: loss 0.703 acc: 0.703\n",
    "\n",
    "# PCA only on the collection\n",
    "# Train results: loss 0.046 acc: 0.046\n",
    "# 45/45 [==============================] - 0s 3ms/step - loss: 0.6707 - mean_squared_error: 0.6707\n",
    "# Validation results: loss 0.671 acc: 0.671\n",
    "# 45/45 [==============================] - 0s 3ms/step - loss: 0.6778 - mean_squared_error: 0.6778\n",
    "# Test results: loss 0.678 acc: 0.678"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlycu3.8)",
   "language": "python",
   "name": "mlycu3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}