{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0e06be",
   "metadata": {},
   "source": [
    "# CLTK Tokenization Comparison with NLTK\n",
    "This notebook aims to show one of several reasons why CLTK outperforms NLTK.\n",
    "Many, non-classical NLP users may wonder why we need a separate library (such as CLTK) if modern NLP methods aim to be generalizable, and there are readily available libraries honed on the problems of modern languages (such as NLTK).\n",
    "\n",
    "Here we compare NLTK and CLTK tokenizers and demonstrate how CLTK functionality can reduce the problem space and help build better, robust models by coalescing morphological variations where appropriate.\n",
    "\n",
    "We will examine the Tesserae Latin corpus (https://github.com/cltk/lat_text_tesserae) and process it using CLTK and NLT functions and compare the results. We choose the Tesserae corpus because it is very clean and does not suffer from OCR errors, nor sloppy font conversions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46259cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from cltk import NLP\n",
    "from cltk.alphabet.lat import normalize_lat\n",
    "from cltk.sentence.lat import LatinPunktSentenceTokenizer\n",
    "from cltk.tokenizers.lat.lat import LatinWordTokenizer\n",
    "from cltk.tokenizers import LatinTokenizationProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc8f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokens = Counter()\n",
    "cltk_tokens = Counter()\n",
    "cltk_jvrtokens = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3807a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tesserae file format has each line starting with metadata like:\n",
    "# <apul.met. 1.1> At ego tibi sermone isto Milesio varias fabulas ...\n",
    "# So we'll remove this:\n",
    "\n",
    "ANY_ANGLE = re.compile(\"<.[^>]+>\") # used to remove tesserae metadata \n",
    "\n",
    "def swallow(text, pattern_matcher):\n",
    "    \"\"\"Given a body of text and a pattern, swallow the text occuring just inside the pattern\"\"\"\n",
    "    idx_to_omit = []\n",
    "    for item in pattern_matcher.finditer(text):\n",
    "        idx_to_omit.insert(0, item.span())\n",
    "    for start, end in idx_to_omit:\n",
    "        text = text[:start] + text[end:]\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37542ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762 tesserae corpus files\n"
     ]
    }
   ],
   "source": [
    "tesserae = glob(os.path.expanduser('~/cltk_data/latin/text/latin_text_tesserae/texts/*.tess'))\n",
    "print(f\"{len(tesserae)} tesserae corpus files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bce96eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_tokenizer = LatinWordTokenizer()\n",
    "sent_toker = LatinPunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a524dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 762/762 [05:18<00:00,  2.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(tesserae, total=len(tesserae)):\n",
    "    with open (file, 'rt') as fin:\n",
    "        text = fin.read()\n",
    "        text = swallow(text, ANY_ANGLE)\n",
    "        for token in word_tokenize(text):\n",
    "            nltk_tokens.update({token : 1})  \n",
    "        for token in latin_tokenizer.tokenize(text):\n",
    "            cltk_tokens.update({token : 1})  \n",
    "        text = normalize_lat(text, drop_accents=True, \n",
    "                                drop_macrons=True,\n",
    "                                jv_replacement=True,\n",
    "                                ligature_replacement=True)    \n",
    "        for token in latin_tokenizer.tokenize(text):\n",
    "            cltk_jvrtokens.update({token : 1})              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf25e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Latin Tesserae corpus:\n",
      "NLTK.word_tokenize: 371,334 distinct tokens, 8,083,360 total tokens\n",
      "CLTK.latin_tokenizer.tokenize: 344,635 distinct tokens, 8,218,743 total tokens\n",
      "CLTK.latin_tokenizer w/normalization, JV replacement, demacronization, ligature replacement: \n",
      "332,764 distinct tokens, 8,218,778 total tokens\n",
      "CLTK reduces Tesserae word/token dimensionality: 10.387%\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Latin Tesserae corpus:\")\n",
    "print(f\"NLTK.word_tokenize: {len(nltk_tokens):,} distinct tokens, {sum(nltk_tokens.values()):,} total tokens\")\n",
    "print(f\"CLTK.latin_tokenizer.tokenize: {len(cltk_tokens):,} distinct tokens, {sum(cltk_tokens.values()):,} total tokens\")\n",
    "print(f\"CLTK.latin_tokenizer w/normalization, JV replacement, demacronization, ligature replacement: \\n{len(cltk_jvrtokens):,} distinct tokens, {sum(cltk_jvrtokens.values()):,} total tokens\")\n",
    "print(f\"CLTK reduces Tesserae word/token dimensionality: {100* (1 - (len(cltk_jvrtokens) / len(nltk_tokens))):.3f}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30656096",
   "metadata": {},
   "source": [
    "## Note the dimensionality reduction above doesn't include reductions that can be made by ignoring case, but this is an application level choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c8dfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower casing NLTK tokens: 337,007 vs. 371,334\n",
      "A 9.244% reduction\n",
      "Lower casing CLTK tokenization w/normalization, JV replacement, demacronization, ligature replacement: \n",
      "299,512 vs. 332,764\n",
      "A 9.993% reduction\n"
     ]
    }
   ],
   "source": [
    "nltk_tok_lower = set([tok.lower() for tok in nltk_tokens])\n",
    "cltk_jvrtokens_lower = set([tok.lower() for tok in cltk_jvrtokens])\n",
    "len(nltk_tok_lower), len(cltk_jvrtokens_lower)\n",
    "print(f\"Lower casing NLTK tokens: {len(nltk_tok_lower):,} vs. {len(nltk_tokens):,}\")\n",
    "print(f\"A {100* (1 - (len(nltk_tok_lower) / len(nltk_tokens))):.3f}% reduction\")\n",
    "print(f\"Lower casing CLTK tokenization w/normalization, JV replacement, demacronization, ligature replacement: \\n{len(cltk_jvrtokens_lower):,} vs. {len(cltk_jvrtokens):,}\")\n",
    "print(f\"A {100* (1 - (len(cltk_jvrtokens_lower) / len(cltk_jvrtokens))):.3f}% reduction\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a400800d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlycu3.8)",
   "language": "python",
   "name": "mlycu3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
