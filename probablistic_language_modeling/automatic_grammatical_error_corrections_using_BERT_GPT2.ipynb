{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Grammatical Error Correction using BERT and GPT2\n",
    "In this notebook, we'll look at word probabilities, sentence probabilities and how both can assist with improved spell checking and automatic grammatical error correction.\n",
    "This notebook fleshes out, expands and clarifies some ideas mentioned in:\n",
    "* `The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction` by Dimitris Alikaniotis, Vipul Raheja [https://arxiv.org/abs/1906.01733]\n",
    "* `BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model` by Alex Wang, Kyunghyun Cho [https://arxiv.org/pdf/1902.04094.pdf]\n",
    "\n",
    "### TL,DR;\n",
    "* \"We extract the probability of a sentence from BERT, by iteratively masking every word in the sentence and then summing the log probabilities. While this approach is far from ideal, it has been shown (Wang and Cho, 2019) that it approximates the log-likelihood of a sentence.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll be using free versions of LanguageTool to spot problem areas:\n",
    "Open a terminal, install docker, and then run:\n",
    "\n",
    "`docker pull erikvl87/languagetool`\n",
    "\n",
    "`docker run --rm -p 8010:8010 erikvl87/languagetool`\n",
    "\n",
    "This will allow you to hit the grammar check endpoint in sections of the code below\n",
    "\n",
    "#### The paid version is undoubtedly worth it.\n",
    "\n",
    "We'll call the service and check for problems to fix using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "LOG=logging.getLogger(\"probas\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logging.disable(logging.INFO)\n",
    "import json\n",
    "from pprint import pprint\n",
    "import requests\n",
    "from itertools import chain\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Add parent dir, so we can access our common code\n",
    "import os, sys, inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "from mlyoucanuse.bert_fun import (\n",
    "    get_alternate_words, \n",
    "    get_word_probabilities,\n",
    "    get_word_in_sentence_probability, \n",
    "    sum_log_probabilities)\n",
    "from mlyoucanuse.gpt2_fun import predict_next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence(sentence):\n",
    "    \"\"\"Helper method to check sentences using languagetool\"\"\"\n",
    "    res = requests.post('http://localhost:8010/v2/check', data= f\"language=en-US&text={sentence}\")    \n",
    "    obj = json.loads(res.content.decode('utf-8'))\n",
    "    return obj.get('matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0220 00:28:30.160527 4581219776 <ipython-input-4-89a9450b03ab>:5] Done!\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "bert_model = BertForMaskedLM.from_pretrained(\"bert-large-cased-whole-word-masking\")\n",
    "bert_model.eval()\n",
    "logging.disable(logging.NOTSET)\n",
    "LOG.info('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The follow example sentence has an error, can you spot it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_sent_1 = 'I am looking forway to see you.' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'context': {'length': 6,\n",
      "              'offset': 13,\n",
      "              'text': 'I am looking forway to see you.'},\n",
      "  'contextForSureMatch': 0,\n",
      "  'ignoreForIncompleteSentence': False,\n",
      "  'length': 6,\n",
      "  'message': 'Possible spelling mistake found',\n",
      "  'offset': 13,\n",
      "  'replacements': [{'value': 'Norway'},\n",
      "                   {'value': 'foray'},\n",
      "                   {'value': 'for way'}],\n",
      "  'rule': {'category': {'id': 'TYPOS', 'name': 'Possible Typo'},\n",
      "           'description': 'Possible spelling mistake',\n",
      "           'id': 'MORFOLOGIK_RULE_EN_US',\n",
      "           'issueType': 'misspelling'},\n",
      "  'sentence': 'I am looking forway to see you.',\n",
      "  'shortMessage': 'Spelling mistake',\n",
      "  'type': {'typeName': 'Other'}}]\n"
     ]
    }
   ],
   "source": [
    "pprint(check_sentence(error_sent_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 4th word is a problem, let's mask it and see BERT's top 5 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('up', 0.5737241506576538),\n",
       " ('forward', 0.27456167340278625),\n",
       " ('down', 0.038530636578798294),\n",
       " ('back', 0.01723042130470276),\n",
       " ('happy', 0.015939364209771156)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_alternate_words(error_sent_1, word_number=4, bert_tokenizer=bert_tokenizer, bert_model=bert_model, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our winner is in second place, perhaps BERT can look at the whole context and give a better decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I', 'am', 'looking', 'up', 'to', 'see', 'you.'],\n",
       " (('I', ('I',), (0.9985413551330566,)),\n",
       "  ('am', ('am',), (0.08395612984895706,)),\n",
       "  ('looking', ('looking',), (0.006572773680090904,)),\n",
       "  ('up', ('up',), (0.5737241506576538,)),\n",
       "  ('to', ('to',), (0.9950821399688721,)),\n",
       "  ('see', ('see',), (0.26060330867767334,)),\n",
       "  ('you', ('you',), (0.00461436016485095,)),\n",
       "  ('.', ('.',), (0.9470862150192261,))),\n",
       " 21.999381840501748)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = error_sent_1.split()\n",
    "tmp[3] = 'up'\n",
    "res = get_word_probabilities(' '.join(tmp), bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "tmp, res, sum_log_probabilities(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I', 'am', 'looking', 'forward', 'to', 'see', 'you.'],\n",
       " (('I', ('I',), (0.9981794357299805,)),\n",
       "  ('am', ('am',), (0.4198238253593445,)),\n",
       "  ('looking', ('looking',), (0.9980196952819824,)),\n",
       "  ('forward', ('forward',), (0.27456167340278625,)),\n",
       "  ('to', ('to',), (0.9999183416366577,)),\n",
       "  ('see', ('see',), (0.0017120845150202513,)),\n",
       "  ('you', ('you',), (0.07673513889312744,)),\n",
       "  ('.', ('.',), (0.9251171350479126,))),\n",
       " 25.66170175304526)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = error_sent_1.split()\n",
    "tmp[3] = 'forward'\n",
    "res= get_word_probabilities(' '.join(tmp), bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "tmp, res, sum_log_probabilities(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOM, context is king"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check Language Tool's suggestions against BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for', '##ay']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize('foray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1111, 4164]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.encode('foray', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I', 'am', 'looking', 'foray', 'to', 'see', 'you.'],\n",
       " (('I', ('I',), (0.997231662273407,)),\n",
       "  ('am', ('am',), (0.16062967479228973,)),\n",
       "  ('looking', ('looking',), (0.0022117383778095245,)),\n",
       "  ('foray', ('for', '##ay'), (0.07856044173240662, 1.936624443032997e-07)),\n",
       "  ('to', ('to',), (0.9439206719398499,)),\n",
       "  ('see', ('see',), (0.033311937004327774,)),\n",
       "  ('you', ('you',), (0.33044958114624023,)),\n",
       "  ('.', ('.',), (0.9541561603546143,))),\n",
       " 10.886311394188859)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = error_sent_1.split()\n",
    "tmp[3] = 'foray'\n",
    "res = get_word_probabilities(' '.join(tmp), bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "tmp, res, sum_log_probabilities(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I', 'am', 'looking', 'for way', 'to', 'see', 'you.'],\n",
       " (('I', ('I',), (0.9958599209785461,)),\n",
       "  ('am', ('am',), (0.1605089008808136,)),\n",
       "  ('looking', ('looking',), (0.7438622713088989,)),\n",
       "  ('for', ('for',), (3.939509952033404e-06,)),\n",
       "  ('way', ('way',), (0.003746665082871914,)),\n",
       "  ('to', ('to',), (0.9703219532966614,)),\n",
       "  ('see', ('see',), (0.04287201166152954,)),\n",
       "  ('you', ('you',), (0.16430877149105072,)),\n",
       "  ('.', ('.',), (0.9581384062767029,))),\n",
       " 16.257300041524807)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = error_sent_1.split()\n",
    "tmp[3] = 'for way'\n",
    "res = get_word_probabilities(' '.join(tmp), bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "tmp, res, sum_log_probabilities(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I', 'am', 'looking', 'Norway', 'to', 'see', 'you.'],\n",
       " (('I', ('I',), (0.9986119270324707,)),\n",
       "  ('am', ('am',), (0.13397079706192017,)),\n",
       "  ('looking', ('looking',), (0.00010536557238083333,)),\n",
       "  ('Norway', ('Norway',), (1.1956956313952105e-08,)),\n",
       "  ('to', ('to',), (0.9202854633331299,)),\n",
       "  ('see', ('see',), (0.01011697482317686,)),\n",
       "  ('you', ('you',), (0.00849774107336998,)),\n",
       "  ('.', ('.',), (0.9501535296440125,))),\n",
       " -2.0658867724002343)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = error_sent_1.split()\n",
    "tmp[3] = 'Norway'\n",
    "res = get_word_probabilities(' '.join(tmp), bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "tmp, res, sum_log_probabilities(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlyoucanuse.gpt2_fun import predict_next_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 takes a shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(logging.INFO)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "gpt2_model.eval()\n",
    "logging.disable(logging.NOTSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('forward', 0.3665640652179718),\n",
       " ('for', 0.35346919298171997),\n",
       " ('to', 0.08423731476068497))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token('I am looking ', gpt2_model, gpt2_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 may be more accurate because it was trained a lot more data, however:\n",
    "* it can only predict next token\n",
    "* it doesn't provide a masked word prediction like BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some classes of errors we'll cover below:\n",
    "* Real word spelling errors; e.g. not OOV\n",
    "* Errors involving a missing word\n",
    "* Errors of an extra word\n",
    "* Errors of agreement\n",
    "* Errors of verb form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real word spelling errors; e.g. not OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('them', 0.2891930639743805),\n",
       " ('it', 0.2877180278301239),\n",
       " ('these', 0.034848809242248535),\n",
       " ('everything', 0.03310466185212135),\n",
       " ('this', 0.028820807114243507)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_7 = \"We can order then directly from the web.\"\n",
    "err_7c = \"We can order them directly from the web.\"\n",
    "\n",
    "get_alternate_words(err_7, word_number=4, bert_tokenizer=bert_tokenizer, bert_model=bert_model, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('We can order then directly from the web.',\n",
       " (('We', ('We',), (0.1670190542936325,)),\n",
       "  ('can', ('can',), (0.15270596742630005,)),\n",
       "  ('order', ('order',), (0.0013151239836588502,)),\n",
       "  ('then', ('then',), (3.236498741898686e-05,)),\n",
       "  ('directly', ('directly',), (0.0045602815225720406,)),\n",
       "  ('from', ('from',), (0.4603978991508484,)),\n",
       "  ('the', ('the',), (0.9359720349311829,)),\n",
       "  ('web', ('web',), (9.6403155112057e-06,)),\n",
       "  ('.', ('.',), (0.970289945602417,))),\n",
       " 2.9934637969274336)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_7_res = get_word_probabilities(err_7, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_7, err_7_res, sum_log_probabilities(err_7_res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('We can order them directly from the web.',\n",
       " (('We', ('We',), (0.11701209098100662,)),\n",
       "  ('can', ('can',), (0.6486707925796509,)),\n",
       "  ('order', ('order',), (0.0020630417857319117,)),\n",
       "  ('them', ('them',), (0.2891930639743805,)),\n",
       "  ('directly', ('directly',), (0.5851895213127136,)),\n",
       "  ('from', ('from',), (0.5881755948066711,)),\n",
       "  ('the', ('the',), (0.8397545218467712,)),\n",
       "  ('web', ('web',), (0.00017660936282481998,)),\n",
       "  ('.', ('.',), (0.9594199657440186,))),\n",
       " 21.51979759853427)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_7c_res = get_word_probabilities(err_7c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_7c, err_7c_res, sum_log_probabilities(err_7c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failure: sadly BERT doesn't know yoga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('human', 0.4340229332447052),\n",
       " ('daily', 0.27425795793533325),\n",
       " ('all', 0.13561923801898956),\n",
       " ('everyday', 0.040244799107313156),\n",
       " ('a', 0.036808937788009644)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# real word spelling error; e.g. not OOV\n",
    "err_6 = \"Yoga brings peace and vitality to you life.\"\n",
    "err_6c = \"Yoga brings peace and vitality to your life.\"\n",
    "\n",
    "err_6_res = get_word_probabilities(err_6, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_6c_res = get_word_probabilities(err_6c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "\n",
    "get_alternate_words(err_6, word_number=7, bert_tokenizer=bert_tokenizer, bert_model=bert_model, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.532024058955813e-09,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_in_sentence_probability(err_6, 'your', bert_model, bert_tokenizer,  position=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.50075166516217e-07,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_in_sentence_probability(err_6, 'you', bert_model=bert_model, bert_tokenizer=bert_tokenizer, position=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Yoga brings peace and vitality to you life.',\n",
       " (('Yoga', ('Yoga',), (3.380464477231726e-05,)),\n",
       "  ('brings', ('brings',), (0.7734541893005371,)),\n",
       "  ('peace', ('peace',), (0.03682466223835945,)),\n",
       "  ('and', ('and',), (0.9945341348648071,)),\n",
       "  ('vitality',\n",
       "   ('vital', '##ity'),\n",
       "   (0.006828881334513426, 0.040755514055490494)),\n",
       "  ('to', ('to',), (0.5773847103118896,)),\n",
       "  ('you', ('you',), (1.3096871498419205e-06,)),\n",
       "  ('life', ('life',), (9.335688810097054e-05,)),\n",
       "  ('.', ('.',), (0.9591231346130371,))),\n",
       " 0.5902883172001783)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_6, err_6_res, sum_log_probabilities(err_6_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Yoga brings peace and vitality to your life.',\n",
       " (('Yoga', ('Yoga',), (9.048711945069954e-05,)),\n",
       "  ('brings', ('brings',), (0.9053715467453003,)),\n",
       "  ('peace', ('peace',), (0.11162364482879639,)),\n",
       "  ('and', ('and',), (0.9985194802284241,)),\n",
       "  ('vitality',\n",
       "   ('vital', '##ity'),\n",
       "   (0.027336645871400833, 0.10959531366825104)),\n",
       "  ('to', ('to',), (0.7094688415527344,)),\n",
       "  ('your', ('your',), (0.004815567284822464,)),\n",
       "  ('life', ('life',), (0.21725884079933167,)),\n",
       "  ('.', ('.',), (0.9960422515869141,))),\n",
       " 21.42762560058551)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_6c, err_6c_res, sum_log_probabilities(err_6c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors involving a missing word\n",
    "(challenging to assess, but technically possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"I'm not sure what I'm up tomorrow.\",\n",
       " (('I', ('I',), (0.9999358654022217,)),\n",
       "  (\"'m\", (\"'\", 'm'), (0.9951947331428528, 0.9977612495422363)),\n",
       "  ('not', ('not',), (0.9959115982055664,)),\n",
       "  ('sure', ('sure',), (0.9941648840904236,)),\n",
       "  ('what', ('what',), (0.29322049021720886,)),\n",
       "  ('I', ('I',), (0.9999940395355225,)),\n",
       "  (\"'m\", (\"'\", 'm'), (0.04088006541132927, 0.00016173844051081687)),\n",
       "  ('up', ('up',), (2.425032107566949e-05,)),\n",
       "  ('tomorrow', ('tomorrow',), (2.398638798695174e-08,)),\n",
       "  ('.', ('.',), (0.980730414390564,))),\n",
       " 13.899174085168003)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_2 = \"I'm not sure what I'm up tomorrow.\"\n",
    "err_2c = \"I'm not sure what I'm up to tomorrow.\"\n",
    "\n",
    "err_2_res =  get_word_probabilities(err_2, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_2, err_2_res, sum_log_probabilities(err_2_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"I'm not sure what I'm up to tomorrow.\",\n",
       " (('I', ('I',), (0.9999393224716187,)),\n",
       "  (\"'m\", (\"'\", 'm'), (0.9969546794891357, 0.9985707998275757)),\n",
       "  ('not', ('not',), (0.9961541295051575,)),\n",
       "  ('sure', ('sure',), (0.994225800037384,)),\n",
       "  ('what', ('what',), (0.9878880381584167,)),\n",
       "  ('I', ('I',), (0.9999959468841553,)),\n",
       "  (\"'m\", (\"'\", 'm'), (0.8839161396026611, 0.9024779200553894)),\n",
       "  ('up', ('up',), (0.5902553796768188,)),\n",
       "  ('to', ('to',), (0.5820885300636292,)),\n",
       "  ('tomorrow', ('tomorrow',), (0.0015056264819577336,)),\n",
       "  ('.', ('.',), (0.9864518642425537,))),\n",
       " 52.034313559054446)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_2c_res= get_word_probabilities( err_2c , bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_2c, err_2c_res, sum_log_probabilities(err_2c_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I am psychologist.',\n",
       " (('I', ('I',), (0.9858765006065369,)),\n",
       "  ('am', ('am',), (0.6945590376853943,)),\n",
       "  ('psychologist', ('psychologist',), (4.13914813179872e-06,)),\n",
       "  ('.', ('.',), (0.8961634635925293,))),\n",
       " 5.53732544369199)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_3 = \"I am psychologist.\"\n",
    "err_3c = \"I am a psychologist.\"\n",
    "\n",
    "err_3_res= get_word_probabilities(err_3, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_3, err_3_res, sum_log_probabilities(err_3_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I am a psychologist.',\n",
       " (('I', ('I',), (0.9993333220481873,)),\n",
       "  ('am', ('am',), (0.6896260380744934,)),\n",
       "  ('a', ('a',), (0.987400233745575,)),\n",
       "  ('psychologist', ('psychologist',), (0.0009351802873425186,)),\n",
       "  ('.', ('.',), (0.9536884427070618,))),\n",
       " 15.618708943693967)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_3c_res= get_word_probabilities( err_3c , bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_3c, err_3c_res, sum_log_probabilities(err_3c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors of an extra word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Why is do they appear in this particular section?',\n",
       " (('Why', ('Why',), (0.16198164224624634,)),\n",
       "  ('is', ('is',), (6.46511180093512e-05,)),\n",
       "  ('do', ('do',), (7.278690645762254e-07,)),\n",
       "  ('they', ('they',), (0.19688940048217773,)),\n",
       "  ('appear', ('appear',), (0.2648281753063202,)),\n",
       "  ('in', ('in',), (0.9802661538124084,)),\n",
       "  ('this', ('this',), (0.2701944410800934,)),\n",
       "  ('particular', ('particular',), (0.07991039007902145,)),\n",
       "  ('section', ('section',), (0.0006710817688144743,)),\n",
       "  ('?', ('?',), (0.9710816144943237,))),\n",
       " 6.306634272865201)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_4 = \"Why is do they appear in this particular section?\"\n",
    "err_4c = \"Why do they appear in this particular section?\"\n",
    "\n",
    "err_4_res= get_word_probabilities(err_4, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_4, err_4_res, sum_log_probabilities(err_4_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Why do they appear in this particular section?',\n",
       " (('Why', ('Why',), (0.7230836749076843,)),\n",
       "  ('do', ('do',), (0.5018960237503052,)),\n",
       "  ('they', ('they',), (0.3515675365924835,)),\n",
       "  ('appear', ('appear',), (0.25058338046073914,)),\n",
       "  ('in', ('in',), (0.9878813028335571,)),\n",
       "  ('this', ('this',), (0.5135653018951416,)),\n",
       "  ('particular', ('particular',), (0.09514071047306061,)),\n",
       "  ('section', ('section',), (0.0003888505743816495,)),\n",
       "  ('?', ('?',), (0.9980717897415161,))),\n",
       " 27.118407410420453)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_4c_res= get_word_probabilities(err_4c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_4c, err_4c_res, sum_log_probabilities(err_4c_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Is our youth really in in such a state of disrepair?',\n",
       " (('Is', ('Is',), (0.5408534407615662,)),\n",
       "  ('our', ('our',), (0.10689887404441833,)),\n",
       "  ('youth', ('youth',), (1.3555451005231589e-05,)),\n",
       "  ('really', ('really',), (0.004895591177046299,)),\n",
       "  ('in', ('in',), (8.652749966131523e-05,)),\n",
       "  ('in', ('in',), (0.02773268148303032,)),\n",
       "  ('such', ('such',), (0.8100823760032654,)),\n",
       "  ('a', ('a',), (0.9965115189552307,)),\n",
       "  ('state', ('state',), (0.9993855953216553,)),\n",
       "  ('of', ('of',), (0.9995771050453186,)),\n",
       "  ('disrepair',\n",
       "   ('di', '##s', '##re', '##pair'),\n",
       "   (0.13752403855323792,\n",
       "    0.048513058573007584,\n",
       "    0.007498561404645443,\n",
       "    0.012231351807713509)),\n",
       "  ('?', ('?',), (0.9984367489814758,))),\n",
       " 22.235348126878584)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_5 = \"Is our youth really in in such a state of disrepair?\"\n",
    "err_5c = \"Is our youth really in such a state of disrepair?\"\n",
    "\n",
    "err_5_res= get_word_probabilities(err_5, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_5, err_5_res, sum_log_probabilities(err_5_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Is our youth really in such a state of disrepair?',\n",
       " (('Is', ('Is',), (0.9543763399124146,)),\n",
       "  ('our', ('our',), (0.12623824179172516,)),\n",
       "  ('youth', ('youth',), (7.61529736337252e-06,)),\n",
       "  ('really', ('really',), (0.11097059398889542,)),\n",
       "  ('in', ('in',), (0.9997593760490417,)),\n",
       "  ('such', ('such',), (0.99972003698349,)),\n",
       "  ('a', ('a',), (0.9996761083602905,)),\n",
       "  ('state', ('state',), (0.9997681975364685,)),\n",
       "  ('of', ('of',), (0.9999357461929321,)),\n",
       "  ('disrepair',\n",
       "   ('di', '##s', '##re', '##pair'),\n",
       "   (0.07216285169124603,\n",
       "    0.030781781300902367,\n",
       "    0.008856026455760002,\n",
       "    0.009444918483495712)),\n",
       "  ('?', ('?',), (0.9991154074668884,))),\n",
       " 32.87163616218337)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_5c_res= get_word_probabilities(err_5c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_5c, err_5c_res, sum_log_probabilities(err_5c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors of Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I awaits your response.',\n",
       " (('I', ('I',), (0.0034313967917114496,)),\n",
       "  ('awaits',\n",
       "   ('a', '##wai', '##ts'),\n",
       "   (0.0020877665374428034, 0.5089987516403198, 2.2291551431408152e-05)),\n",
       "  ('your', ('your',), (0.005505810026079416,)),\n",
       "  ('response', ('response',), (0.03314106538891792,)),\n",
       "  ('.', ('.',), (0.9816052317619324,))),\n",
       " 0.37563092539703113)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_8 = \"I awaits your response.\"\n",
    "err_8c = \"I await your response.\"\n",
    "\n",
    "err_8_res= get_word_probabilities(err_8, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_8, err_8_res, sum_log_probabilities(err_8_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I await your response.',\n",
       " (('I', ('I',), (0.7317708730697632,)),\n",
       "  ('await',\n",
       "   ('a', '##wai', '##t'),\n",
       "   (0.0020877665374428034, 0.5089987516403198, 0.012238427996635437)),\n",
       "  ('your', ('your',), (0.014312163926661015,)),\n",
       "  ('response', ('response',), (0.014434893615543842,)),\n",
       "  ('.', ('.',), (0.9868567585945129,))),\n",
       " 12.175776196388547)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_8c_res= get_word_probabilities(err_8c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_8c, err_8c_res, sum_log_probabilities(err_8c_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The first of these scientist begin in January.',\n",
       " (('The', ('The',), (0.9965081810951233,)),\n",
       "  ('first', ('first',), (0.004373201169073582,)),\n",
       "  ('of', ('of',), (0.9866845011711121,)),\n",
       "  ('these', ('these',), (0.057628609240055084,)),\n",
       "  ('scientist', ('scientist',), (1.0894053437127127e-09,)),\n",
       "  ('begin', ('begin',), (0.010585297830402851,)),\n",
       "  ('in', ('in',), (0.9678221344947815,)),\n",
       "  ('January', ('January',), (0.0036275957245379686,)),\n",
       "  ('.', ('.',), (0.9448446035385132,))),\n",
       " 2.2490826196956517)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_9 = \"The first of these scientist begin in January.\"\n",
    "err_9c = \"The first of these scientists begin in January.\"\n",
    "err_9_res = get_word_probabilities(err_9, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_9, err_9_res, sum_log_probabilities(err_9_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The first of these scientists begin in January.',\n",
       " (('The', ('The',), (0.9947749376296997,)),\n",
       "  ('first', ('first',), (0.0005680953036062419,)),\n",
       "  ('of', ('of',), (0.91595858335495,)),\n",
       "  ('these', ('these',), (0.05670550465583801,)),\n",
       "  ('scientists', ('scientists',), (8.002268145901326e-07,)),\n",
       "  ('begin', ('begin',), (0.0017818311462178826,)),\n",
       "  ('in', ('in',), (0.8879834413528442,)),\n",
       "  ('January', ('January',), (0.001778475008904934,)),\n",
       "  ('.', ('.',), (0.9370677471160889,))),\n",
       " 4.126118188968048)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_9c_res = get_word_probabilities(err_9c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_9c, err_9c_res, sum_log_probabilities(err_9c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors with Verb Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Brent would often became stunned by resentment.',\n",
       " (('Brent', ('Brent',), (5.147402043803595e-05,)),\n",
       "  ('would', ('would',), (0.0011423516552895308,)),\n",
       "  ('often', ('often',), (0.0006808876059949398,)),\n",
       "  ('became', ('became',), (1.685629649728071e-05,)),\n",
       "  ('stunned', ('stunned',), (0.00016873223648872226,)),\n",
       "  ('by', ('by',), (0.32671085000038147,)),\n",
       "  ('resentment', ('resentment',), (7.069127605063841e-05,)),\n",
       "  ('.', ('.',), (0.9559879899024963,))),\n",
       " -17.498713297129644)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_10 = \"Brent would often became stunned by resentment.\"\n",
    "err_10c = \"Brent would often become stunned by resentment.\"\n",
    "err_10_res= get_word_probabilities(err_10, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_10, err_10_res, sum_log_probabilities(err_10_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Brent would often become stunned by resentment.',\n",
       " (('Brent', ('Brent',), (6.225280958460644e-05,)),\n",
       "  ('would', ('would',), (0.372565895318985,)),\n",
       "  ('often', ('often',), (0.022015467286109924,)),\n",
       "  ('become', ('become',), (0.028336457908153534,)),\n",
       "  ('stunned', ('stunned',), (8.714829164091498e-05,)),\n",
       "  ('by', ('by',), (0.3506300151348114,)),\n",
       "  ('resentment', ('resentment',), (5.4146814363775775e-05,)),\n",
       "  ('.', ('.',), (0.9466897249221802,))),\n",
       " -1.4844211944607055)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_10c_res= get_word_probabilities(err_10c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_10c, err_10c_res, sum_log_probabilities(err_10c_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I having mostly been moving flat.',\n",
       " (('I', ('I',), (0.005292738322168589,)),\n",
       "  ('having', ('having',), (6.289655721047893e-05,)),\n",
       "  ('mostly', ('mostly',), (0.005705267656594515,)),\n",
       "  ('been', ('been',), (0.5970264077186584,)),\n",
       "  ('moving', ('moving',), (0.00012258144852239639,)),\n",
       "  ('flat', ('flat',), (9.995254913519602e-06,)),\n",
       "  ('.', ('.',), (0.8984378576278687,))),\n",
       " -8.98863935388643)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_11 = \"I having mostly been moving flat.\"\n",
    "err_11c = \"I have mostly been moving flat.\"\n",
    "err_11_res= get_word_probabilities(err_11, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_11, err_11_res, sum_log_probabilities(err_11_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I have mostly been moving flat.',\n",
       " (('I', ('I',), (0.11393590271472931,)),\n",
       "  ('have', ('have',), (0.41972973942756653,)),\n",
       "  ('mostly', ('mostly',), (0.005898055154830217,)),\n",
       "  ('been', ('been',), (0.9094554781913757,)),\n",
       "  ('moving', ('moving',), (0.00014484622806776315,)),\n",
       "  ('flat', ('flat',), (9.372281965625007e-06,)),\n",
       "  ('.', ('.',), (0.9684615731239319,))),\n",
       " 3.5182476527900794)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_11c_res= get_word_probabilities(err_11c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_11c, err_11c_res, sum_log_probabilities(err_11c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all for now!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
