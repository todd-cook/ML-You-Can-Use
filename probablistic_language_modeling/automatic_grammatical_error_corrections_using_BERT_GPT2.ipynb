{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Grammatical Error Correction using BERT and GPT2\n",
    "In this notebook, we'll look at word probabilities, sentence probabilities and how both can assist with improved spell checking and automatic grammatical error correction.\n",
    "This notebook fleshes out, expands and clarifies some ideas mentioned in:\n",
    "* `The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction` by Dimitris Alikaniotis, Vipul Raheja [https://arxiv.org/abs/1906.01733]\n",
    "* `BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model` by Alex Wang, Kyunghyun Cho [https://arxiv.org/pdf/1902.04094.pdf]\n",
    "\n",
    "### Key Points:\n",
    "* \"We extract the probability of a sentence from BERT, by iteratively masking every word in the sentence and then summing the log probabilities. While this approach is far from ideal, it has been shown (Wang and Cho, 2019) that it approximates the log-likelihood of a sentence.\"\n",
    "\n",
    "Log probabilities are summed so as to avoid issues with underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "LOG = logging.getLogger(\"probas\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logging.disable(logging.INFO)\n",
    "import json\n",
    "from pprint import pprint\n",
    "from itertools import chain\n",
    "\n",
    "import requests\n",
    "from transformers import BertTokenizer, BertForMaskedLM, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Add parent dir, so we can access our common code\n",
    "import os, sys, inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "from mlyoucanuse.bert_fun import (\n",
    "    get_alternate_words, \n",
    "    get_word_probabilities,\n",
    "    get_word_in_sentence_probability, \n",
    "    sum_log_probabilities)\n",
    "from mlyoucanuse.gpt2_fun import predict_next_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll be using free versions of LanguageTool to spot problem areas:\n",
    "Open a terminal, install docker, and then run:\n",
    "\n",
    "`docker pull erikvl87/languagetool`\n",
    "\n",
    "`docker run --rm -p 8010:8010 erikvl87/languagetool`\n",
    "\n",
    "This will allow you to hit the grammar check endpoint in sections of the code below\n",
    "\n",
    "#### The paid version is undoubtedly worth it.\n",
    "\n",
    "We'll call the service and check for problems to fix using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence(sentence):\n",
    "    \"\"\"Helper method to check sentences using languagetool\"\"\"\n",
    "    res = requests.post('http://localhost:8010/v2/check', data= f\"language=en-US&text={sentence}\")    \n",
    "    obj = json.loads(res.content.decode('utf-8'))\n",
    "    return obj.get('matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0224 21:10:24.711054 4448169408 <ipython-input-4-89a9450b03ab>:5] Done!\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "bert_model = BertForMaskedLM.from_pretrained(\"bert-large-cased-whole-word-masking\")\n",
    "bert_model.eval()\n",
    "logging.disable(logging.NOTSET)\n",
    "LOG.info('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The follow example sentence has an error, can you spot it?\n",
    "## `I am looking forway to see you.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'context': {'length': 6,\n",
      "              'offset': 13,\n",
      "              'text': 'I am looking forway to see you.'},\n",
      "  'contextForSureMatch': 0,\n",
      "  'ignoreForIncompleteSentence': False,\n",
      "  'length': 6,\n",
      "  'message': 'Possible spelling mistake found',\n",
      "  'offset': 13,\n",
      "  'replacements': [{'value': 'Norway'},\n",
      "                   {'value': 'foray'},\n",
      "                   {'value': 'for way'}],\n",
      "  'rule': {'category': {'id': 'TYPOS', 'name': 'Possible Typo'},\n",
      "           'description': 'Possible spelling mistake',\n",
      "           'id': 'MORFOLOGIK_RULE_EN_US',\n",
      "           'issueType': 'misspelling'},\n",
      "  'sentence': 'I am looking forway to see you.',\n",
      "  'shortMessage': 'Spelling mistake',\n",
      "  'type': {'typeName': 'Other'}}]\n"
     ]
    }
   ],
   "source": [
    "error_sent_1 = 'I am looking forway to see you.' \n",
    "\n",
    "pprint(check_sentence(error_sent_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 4th word is a problem, let's mask it and see BERT's top 5 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('up', 0.5737241506576538),\n",
       " ('forward', 0.27456167340278625),\n",
       " ('down', 0.038530636578798294),\n",
       " ('back', 0.01723042130470276),\n",
       " ('happy', 0.015939364209771156))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_alternate_words(sentence=error_sent_1, \n",
    "                    word_index=3,\n",
    "                    bert_tokenizer=bert_tokenizer,\n",
    "                    bert_model=bert_model,\n",
    "                    top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our winner is in second place, perhaps BERT can look at the whole context and give us a better decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I', 'am', 'looking', 'up', 'to', 'see', 'you.'],\n",
       " (('I', ('I',), (0.9985413551330566,)),\n",
       "  ('am', ('am',), (0.08395612984895706,)),\n",
       "  ('looking', ('looking',), (0.006572773680090904,)),\n",
       "  ('up', ('up',), (0.5737241506576538,)),\n",
       "  ('to', ('to',), (0.9950821399688721,)),\n",
       "  ('see', ('see',), (0.26060330867767334,)),\n",
       "  ('you', ('you',), (0.00461436016485095,)),\n",
       "  ('.', ('.',), (0.9470862150192261,))),\n",
       " 21.999381840501748)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = error_sent_1.split()\n",
    "tmp[3] = 'up'\n",
    "res = get_word_probabilities(' '.join(tmp), bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "tmp, res, sum_log_probabilities(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I', 'am', 'looking', 'forward', 'to', 'see', 'you.'],\n",
       " (('I', ('I',), (0.9981794357299805,)),\n",
       "  ('am', ('am',), (0.4198238253593445,)),\n",
       "  ('looking', ('looking',), (0.9980196952819824,)),\n",
       "  ('forward', ('forward',), (0.27456167340278625,)),\n",
       "  ('to', ('to',), (0.9999183416366577,)),\n",
       "  ('see', ('see',), (0.0017120845150202513,)),\n",
       "  ('you', ('you',), (0.07673513889312744,)),\n",
       "  ('.', ('.',), (0.9251171350479126,))),\n",
       " 25.66170175304526)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = error_sent_1.split()\n",
    "tmp[3] = 'forward'\n",
    "res = get_word_probabilities(' '.join(tmp), bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "tmp, res, sum_log_probabilities(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice: although `up` had a higher predicted probability than `forward`, one word influences all the other probabilities. Thus, on the whole, taken in context, BERT predicts the correct word in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check Language Tool's suggestions against BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I', 'am', 'looking', 'foray', 'to', 'see', 'you.'],\n",
       " (('I', ('I',), (0.997231662273407,)),\n",
       "  ('am', ('am',), (0.16062967479228973,)),\n",
       "  ('looking', ('looking',), (0.0022117383778095245,)),\n",
       "  ('foray', ('for', '##ay'), (0.07856044173240662, 1.936624443032997e-07)),\n",
       "  ('to', ('to',), (0.9439206719398499,)),\n",
       "  ('see', ('see',), (0.033311937004327774,)),\n",
       "  ('you', ('you',), (0.33044958114624023,)),\n",
       "  ('.', ('.',), (0.9541561603546143,))),\n",
       " 10.886311394188859)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = error_sent_1.split()\n",
    "tmp[3] = 'foray'\n",
    "res = get_word_probabilities(' '.join(tmp), bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "tmp, res, sum_log_probabilities(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I', 'am', 'looking', 'for way', 'to', 'see', 'you.'],\n",
       " (('I', ('I',), (0.9958599209785461,)),\n",
       "  ('am', ('am',), (0.1605089008808136,)),\n",
       "  ('looking', ('looking',), (0.7438622713088989,)),\n",
       "  ('for', ('for',), (3.939509952033404e-06,)),\n",
       "  ('way', ('way',), (0.003746665082871914,)),\n",
       "  ('to', ('to',), (0.9703219532966614,)),\n",
       "  ('see', ('see',), (0.04287201166152954,)),\n",
       "  ('you', ('you',), (0.16430877149105072,)),\n",
       "  ('.', ('.',), (0.9581384062767029,))),\n",
       " 16.257300041524807)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = error_sent_1.split()\n",
    "tmp[3] = 'for way'\n",
    "res = get_word_probabilities(' '.join(tmp), bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "tmp, res, sum_log_probabilities(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I', 'am', 'looking', 'Norway', 'to', 'see', 'you.'],\n",
       " (('I', ('I',), (0.9986119270324707,)),\n",
       "  ('am', ('am',), (0.13397079706192017,)),\n",
       "  ('looking', ('looking',), (0.00010536557238083333,)),\n",
       "  ('Norway', ('Norway',), (1.1956956313952105e-08,)),\n",
       "  ('to', ('to',), (0.9202854633331299,)),\n",
       "  ('see', ('see',), (0.01011697482317686,)),\n",
       "  ('you', ('you',), (0.00849774107336998,)),\n",
       "  ('.', ('.',), (0.9501535296440125,))),\n",
       " -2.0658867724002343)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = error_sent_1.split()\n",
    "tmp[3] = 'Norway'\n",
    "res = get_word_probabilities(' '.join(tmp), bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "tmp, res, sum_log_probabilities(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT does subword tokenization for words OOV (aka beyond its basic whole word range)\n",
    "* Anybody want to guess where this will make for pain and suffering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for', '##ay']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize('foray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1111, 4164]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.encode('foray', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 can also predict the best match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(logging.INFO)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "gpt2_model.eval()\n",
    "logging.disable(logging.NOTSET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `I am looking`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('forward', 0.3665640652179718),\n",
       " ('for', 0.35346919298171997),\n",
       " ('to', 0.08423731476068497))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token('I am looking ', gpt2_model, gpt2_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 may be more accurate because it was trained a lot more data, however:\n",
    "* it can only predict next token\n",
    "* it doesn't provide a masked word prediction like BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's examine some common classes of grammatical errors:\n",
    "* Real word spelling errors; e.g. not OOV\n",
    "* Errors involving a missing word\n",
    "* Errors of an extra word\n",
    "* Errors of agreement\n",
    "* Errors of verb form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real word spelling errors; e.g. not OOV, e.g.\n",
    "## `We can order then directly from the web.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "err_7 = \"We can order then directly from the web.\"\n",
    "err_7c = \"We can order them directly from the web.\"\n",
    "\n",
    "pprint(check_sentence(err_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LanguageTool doesn't detect this error, let's pretend we know where it's at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('them', 0.2891930639743805),\n",
       " ('it', 0.2877180278301239),\n",
       " ('these', 0.034848809242248535),\n",
       " ('everything', 0.03310466185212135),\n",
       " ('this', 0.028820807114243507))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_alternate_words(sentence=err_7,\n",
    "                    word_index=3,\n",
    "                    bert_tokenizer=bert_tokenizer,\n",
    "                    bert_model=bert_model,\n",
    "                    top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('We can order then directly from the web.',\n",
       " (('We', ('We',), (0.1670190542936325,)),\n",
       "  ('can', ('can',), (0.15270596742630005,)),\n",
       "  ('order', ('order',), (0.0013151239836588502,)),\n",
       "  ('then', ('then',), (3.236498741898686e-05,)),\n",
       "  ('directly', ('directly',), (0.0045602815225720406,)),\n",
       "  ('from', ('from',), (0.4603978991508484,)),\n",
       "  ('the', ('the',), (0.9359720349311829,)),\n",
       "  ('web', ('web',), (9.6403155112057e-06,)),\n",
       "  ('.', ('.',), (0.970289945602417,))),\n",
       " 2.9934637969274336)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_7_res = get_word_probabilities(err_7, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_7, err_7_res, sum_log_probabilities(err_7_res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('We can order them directly from the web.',\n",
       " (('We', ('We',), (0.11701209098100662,)),\n",
       "  ('can', ('can',), (0.6486707925796509,)),\n",
       "  ('order', ('order',), (0.0020630417857319117,)),\n",
       "  ('them', ('them',), (0.2891930639743805,)),\n",
       "  ('directly', ('directly',), (0.5851895213127136,)),\n",
       "  ('from', ('from',), (0.5881755948066711,)),\n",
       "  ('the', ('the',), (0.8397545218467712,)),\n",
       "  ('web', ('web',), (0.00017660936282481998,)),\n",
       "  ('.', ('.',), (0.9594199657440186,))),\n",
       " 21.51979759853427)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_7c_res = get_word_probabilities(err_7c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_7c, err_7c_res, sum_log_probabilities(err_7c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another real word spelling error; e.g. not OOV:\n",
    "## `Yoga brings peace and vitality to you life.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# real word spelling error; e.g. not OOV\n",
    "err_6 = \"Yoga brings peace and vitality to you life.\"\n",
    "err_6c = \"Yoga brings peace and vitality to your life.\"\n",
    "\n",
    "pprint(check_sentence(err_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('human', 0.4340229332447052),\n",
       " ('daily', 0.27425795793533325),\n",
       " ('all', 0.13561923801898956),\n",
       " ('everyday', 0.040244799107313156),\n",
       " ('a', 0.036808937788009644))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_6_res = get_word_probabilities(err_6, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_6c_res = get_word_probabilities(err_6c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "\n",
    "get_alternate_words(sentence=err_6, word_index=6, bert_tokenizer=bert_tokenizer, bert_model=bert_model, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3096871498419205e-06,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_in_sentence_probability(sentence=err_6,\n",
    "                                 word='you',\n",
    "                                 bert_model=bert_model,\n",
    "                                 bert_tokenizer=bert_tokenizer,\n",
    "                                 word_index=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.004815567284822464,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_in_sentence_probability(sentence=err_6, \n",
    "                                 word='your',       \n",
    "                                 bert_model=bert_model,\n",
    "                                 bert_tokenizer=bert_tokenizer, \n",
    "                                 word_index=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Yoga brings peace and vitality to you life.',\n",
       " (('Yoga', ('Yoga',), (3.380464477231726e-05,)),\n",
       "  ('brings', ('brings',), (0.7734541893005371,)),\n",
       "  ('peace', ('peace',), (0.03682466223835945,)),\n",
       "  ('and', ('and',), (0.9945341348648071,)),\n",
       "  ('vitality',\n",
       "   ('vital', '##ity'),\n",
       "   (0.006828881334513426, 0.040755514055490494)),\n",
       "  ('to', ('to',), (0.5773847103118896,)),\n",
       "  ('you', ('you',), (1.3096871498419205e-06,)),\n",
       "  ('life', ('life',), (9.335688810097054e-05,)),\n",
       "  ('.', ('.',), (0.9591231346130371,))),\n",
       " 0.5902883172001783)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_6, err_6_res, sum_log_probabilities(err_6_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Yoga brings peace and vitality to your life.',\n",
       " (('Yoga', ('Yoga',), (9.048711945069954e-05,)),\n",
       "  ('brings', ('brings',), (0.9053715467453003,)),\n",
       "  ('peace', ('peace',), (0.11162364482879639,)),\n",
       "  ('and', ('and',), (0.9985194802284241,)),\n",
       "  ('vitality',\n",
       "   ('vital', '##ity'),\n",
       "   (0.027336645871400833, 0.10959531366825104)),\n",
       "  ('to', ('to',), (0.7094688415527344,)),\n",
       "  ('your', ('your',), (0.004815567284822464,)),\n",
       "  ('life', ('life',), (0.21725884079933167,)),\n",
       "  ('.', ('.',), (0.9960422515869141,))),\n",
       " 21.42762560058551)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_6c, err_6c_res, sum_log_probabilities(err_6c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors involving a missing word\n",
    "(challenging to assess, but technically possible)\n",
    "## `I'm not sure what I'm up tomorrow.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "err_2 = \"I'm not sure what I'm up tomorrow.\"\n",
    "err_2c = \"I'm not sure what I'm up to tomorrow.\"\n",
    "\n",
    "pprint(check_sentence(err_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"I'm not sure what I'm up tomorrow.\",\n",
       " (('I', ('I',), (0.9999358654022217,)),\n",
       "  (\"'m\", (\"'\", 'm'), (0.9951947331428528, 0.9977612495422363)),\n",
       "  ('not', ('not',), (0.9959115982055664,)),\n",
       "  ('sure', ('sure',), (0.9941648840904236,)),\n",
       "  ('what', ('what',), (0.29322049021720886,)),\n",
       "  ('I', ('I',), (0.9999940395355225,)),\n",
       "  (\"'m\", (\"'\", 'm'), (0.04088006541132927, 0.00016173844051081687)),\n",
       "  ('up', ('up',), (2.425032107566949e-05,)),\n",
       "  ('tomorrow', ('tomorrow',), (2.398638798695174e-08,)),\n",
       "  ('.', ('.',), (0.980730414390564,))),\n",
       " 13.899174085168003)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_2_res = get_word_probabilities(err_2, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_2, err_2_res, sum_log_probabilities(err_2_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"I'm not sure what I'm up to tomorrow.\",\n",
       " (('I', ('I',), (0.9999393224716187,)),\n",
       "  (\"'m\", (\"'\", 'm'), (0.9969546794891357, 0.9985707998275757)),\n",
       "  ('not', ('not',), (0.9961541295051575,)),\n",
       "  ('sure', ('sure',), (0.994225800037384,)),\n",
       "  ('what', ('what',), (0.9878880381584167,)),\n",
       "  ('I', ('I',), (0.9999959468841553,)),\n",
       "  (\"'m\", (\"'\", 'm'), (0.8839161396026611, 0.9024779200553894)),\n",
       "  ('up', ('up',), (0.5902553796768188,)),\n",
       "  ('to', ('to',), (0.5820885300636292,)),\n",
       "  ('tomorrow', ('tomorrow',), (0.0015056264819577336,)),\n",
       "  ('.', ('.',), (0.9864518642425537,))),\n",
       " 52.034313559054446)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_2c_res = get_word_probabilities(err_2c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_2c, err_2c_res, sum_log_probabilities(err_2c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another missing word example:\n",
    "## `I am psychologist.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "err_3 = \"I am psychologist.\"\n",
    "err_3c = \"I am a psychologist.\"\n",
    "\n",
    "pprint(check_sentence(err_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I am psychologist.',\n",
       " (('I', ('I',), (0.9858765006065369,)),\n",
       "  ('am', ('am',), (0.6945590376853943,)),\n",
       "  ('psychologist', ('psychologist',), (4.13914813179872e-06,)),\n",
       "  ('.', ('.',), (0.8961634635925293,))),\n",
       " 5.53732544369199)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_3_res = get_word_probabilities(err_3, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_3, err_3_res, sum_log_probabilities(err_3_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I am a psychologist.',\n",
       " (('I', ('I',), (0.9993333220481873,)),\n",
       "  ('am', ('am',), (0.6896260380744934,)),\n",
       "  ('a', ('a',), (0.987400233745575,)),\n",
       "  ('psychologist', ('psychologist',), (0.0009351802873425186,)),\n",
       "  ('.', ('.',), (0.9536884427070618,))),\n",
       " 15.618708943693967)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_3c_res = get_word_probabilities(err_3c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_3c, err_3c_res, sum_log_probabilities(err_3c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors of an extra word, e.g.:\n",
    "## `Why is do they appear in this particular section?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'context': {'length': 2,\n",
      "              'offset': 7,\n",
      "              'text': 'Why is do they appear in this particular section?...'},\n",
      "  'contextForSureMatch': -1,\n",
      "  'ignoreForIncompleteSentence': True,\n",
      "  'length': 2,\n",
      "  'message': 'Consider using a past participle here: \"done\".',\n",
      "  'offset': 7,\n",
      "  'replacements': [{'value': 'done'}],\n",
      "  'rule': {'category': {'id': 'GRAMMAR', 'name': 'Grammar'},\n",
      "           'description': \"Agreement: 'been' or 'was' + past tense\",\n",
      "           'id': 'BEEN_PART_AGREEMENT',\n",
      "           'issueType': 'grammar',\n",
      "           'subId': '1'},\n",
      "  'sentence': 'Why is do they appear in this particular section?',\n",
      "  'shortMessage': 'Possible agreement error',\n",
      "  'type': {'typeName': 'Other'}}]\n"
     ]
    }
   ],
   "source": [
    "err_4 = \"Why is do they appear in this particular section?\"\n",
    "err_4c = \"Why do they appear in this particular section?\"\n",
    "\n",
    "pprint(check_sentence(err_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Why is do they appear in this particular section?',\n",
       " (('Why', ('Why',), (0.16198164224624634,)),\n",
       "  ('is', ('is',), (6.46511180093512e-05,)),\n",
       "  ('do', ('do',), (7.278690645762254e-07,)),\n",
       "  ('they', ('they',), (0.19688940048217773,)),\n",
       "  ('appear', ('appear',), (0.2648281753063202,)),\n",
       "  ('in', ('in',), (0.9802661538124084,)),\n",
       "  ('this', ('this',), (0.2701944410800934,)),\n",
       "  ('particular', ('particular',), (0.07991039007902145,)),\n",
       "  ('section', ('section',), (0.0006710817688144743,)),\n",
       "  ('?', ('?',), (0.9710816144943237,))),\n",
       " 6.306634272865201)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_4_res = get_word_probabilities(err_4, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_4, err_4_res, sum_log_probabilities(err_4_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Why do they appear in this particular section?',\n",
       " (('Why', ('Why',), (0.7230836749076843,)),\n",
       "  ('do', ('do',), (0.5018960237503052,)),\n",
       "  ('they', ('they',), (0.3515675365924835,)),\n",
       "  ('appear', ('appear',), (0.25058338046073914,)),\n",
       "  ('in', ('in',), (0.9878813028335571,)),\n",
       "  ('this', ('this',), (0.5135653018951416,)),\n",
       "  ('particular', ('particular',), (0.09514071047306061,)),\n",
       "  ('section', ('section',), (0.0003888505743816495,)),\n",
       "  ('?', ('?',), (0.9980717897415161,))),\n",
       " 27.118407410420453)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_4c_res = get_word_probabilities(err_4c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_4c, err_4c_res, sum_log_probabilities(err_4c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate word errors, e.g.:\n",
    "## `Is our youth really in in such a state of disrepair?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'context': {'length': 5,\n",
      "              'offset': 20,\n",
      "              'text': 'Is our youth really in in such a state of disrepair?'},\n",
      "  'contextForSureMatch': 1,\n",
      "  'ignoreForIncompleteSentence': False,\n",
      "  'length': 5,\n",
      "  'message': 'Possible typo: you repeated a word',\n",
      "  'offset': 20,\n",
      "  'replacements': [{'value': 'in'}],\n",
      "  'rule': {'category': {'id': 'MISC', 'name': 'Miscellaneous'},\n",
      "           'description': \"Word repetition (e.g. 'will will')\",\n",
      "           'id': 'ENGLISH_WORD_REPEAT_RULE',\n",
      "           'issueType': 'duplication'},\n",
      "  'sentence': 'Is our youth really in in such a state of disrepair?',\n",
      "  'shortMessage': 'Word repetition',\n",
      "  'type': {'typeName': 'Other'}}]\n"
     ]
    }
   ],
   "source": [
    "err_5 = \"Is our youth really in in such a state of disrepair?\"\n",
    "err_5c = \"Is our youth really in such a state of disrepair?\"\n",
    "\n",
    "pprint(check_sentence(err_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here, LanguageTool's suggestion is excellent, and it would be hard to detect via BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Is our youth really in in such a state of disrepair?',\n",
       " (('Is', ('Is',), (0.5408534407615662,)),\n",
       "  ('our', ('our',), (0.10689887404441833,)),\n",
       "  ('youth', ('youth',), (1.3555451005231589e-05,)),\n",
       "  ('really', ('really',), (0.004895591177046299,)),\n",
       "  ('in', ('in',), (8.652749966131523e-05,)),\n",
       "  ('in', ('in',), (0.02773268148303032,)),\n",
       "  ('such', ('such',), (0.8100823760032654,)),\n",
       "  ('a', ('a',), (0.9965115189552307,)),\n",
       "  ('state', ('state',), (0.9993855953216553,)),\n",
       "  ('of', ('of',), (0.9995771050453186,)),\n",
       "  ('disrepair',\n",
       "   ('di', '##s', '##re', '##pair'),\n",
       "   (0.13752403855323792,\n",
       "    0.048513058573007584,\n",
       "    0.007498561404645443,\n",
       "    0.012231351807713509)),\n",
       "  ('?', ('?',), (0.9984367489814758,))),\n",
       " 22.235348126878584)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_5_res = get_word_probabilities(err_5, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_5, err_5_res, sum_log_probabilities(err_5_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Is our youth really in such a state of disrepair?',\n",
       " (('Is', ('Is',), (0.9543763399124146,)),\n",
       "  ('our', ('our',), (0.12623824179172516,)),\n",
       "  ('youth', ('youth',), (7.61529736337252e-06,)),\n",
       "  ('really', ('really',), (0.11097059398889542,)),\n",
       "  ('in', ('in',), (0.9997593760490417,)),\n",
       "  ('such', ('such',), (0.99972003698349,)),\n",
       "  ('a', ('a',), (0.9996761083602905,)),\n",
       "  ('state', ('state',), (0.9997681975364685,)),\n",
       "  ('of', ('of',), (0.9999357461929321,)),\n",
       "  ('disrepair',\n",
       "   ('di', '##s', '##re', '##pair'),\n",
       "   (0.07216285169124603,\n",
       "    0.030781781300902367,\n",
       "    0.008856026455760002,\n",
       "    0.009444918483495712)),\n",
       "  ('?', ('?',), (0.9991154074668884,))),\n",
       " 32.87163616218337)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_5c_res = get_word_probabilities(err_5c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_5c, err_5c_res, sum_log_probabilities(err_5c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors of Agreement, e.g.:\n",
    "## `I awaits your response.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'context': {'length': 6, 'offset': 2, 'text': 'I awaits your response.'},\n",
      "  'contextForSureMatch': 0,\n",
      "  'ignoreForIncompleteSentence': False,\n",
      "  'length': 6,\n",
      "  'message': 'Possible agreement error -- use base form here: \"await\".',\n",
      "  'offset': 2,\n",
      "  'replacements': [{'value': 'await'}],\n",
      "  'rule': {'category': {'id': 'GRAMMAR', 'name': 'Grammar'},\n",
      "           'description': 'base form after I/you/we/they',\n",
      "           'id': 'BASE_FORM',\n",
      "           'issueType': 'grammar',\n",
      "           'subId': '1'},\n",
      "  'sentence': 'I awaits your response.',\n",
      "  'shortMessage': '',\n",
      "  'type': {'typeName': 'Other'}}]\n"
     ]
    }
   ],
   "source": [
    "err_8 = \"I awaits your response.\"\n",
    "err_8c = \"I await your response.\"\n",
    "\n",
    "pprint(check_sentence(err_8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another excellent LanguageTool suggestion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I awaits your response.',\n",
       " (('I', ('I',), (0.0034313967917114496,)),\n",
       "  ('awaits',\n",
       "   ('a', '##wai', '##ts'),\n",
       "   (0.0020877665374428034, 0.5089987516403198, 2.2291551431408152e-05)),\n",
       "  ('your', ('your',), (0.005505810026079416,)),\n",
       "  ('response', ('response',), (0.03314106538891792,)),\n",
       "  ('.', ('.',), (0.9816052317619324,))),\n",
       " 0.37563092539703113)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_8_res = get_word_probabilities(err_8, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_8, err_8_res, sum_log_probabilities(err_8_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I await your response.',\n",
       " (('I', ('I',), (0.7317708730697632,)),\n",
       "  ('await',\n",
       "   ('a', '##wai', '##t'),\n",
       "   (0.0020877665374428034, 0.5089987516403198, 0.012238427996635437)),\n",
       "  ('your', ('your',), (0.014312163926661015,)),\n",
       "  ('response', ('response',), (0.014434893615543842,)),\n",
       "  ('.', ('.',), (0.9868567585945129,))),\n",
       " 12.175776196388547)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_8c_res = get_word_probabilities(err_8c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_8c, err_8c_res, sum_log_probabilities(err_8c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another error of agreement:\n",
    "## `The first of these scientist begin in January.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'context': {'length': 15,\n",
      "              'offset': 13,\n",
      "              'text': 'The first of these scientist begin in January.'},\n",
      "  'contextForSureMatch': 9,\n",
      "  'ignoreForIncompleteSentence': True,\n",
      "  'length': 15,\n",
      "  'message': 'Did you mean \"this scientist\" or \"these scientists\"?',\n",
      "  'offset': 13,\n",
      "  'replacements': [{'value': 'this scientist'}, {'value': 'these scientists'}],\n",
      "  'rule': {'category': {'id': 'GRAMMAR', 'name': 'Grammar'},\n",
      "           'description': \"'this' vs. 'these'\",\n",
      "           'id': 'THIS_NNS',\n",
      "           'issueType': 'grammar',\n",
      "           'subId': '3'},\n",
      "  'sentence': 'The first of these scientist begin in January.',\n",
      "  'shortMessage': \"Grammatical problem: use 'this'\",\n",
      "  'type': {'typeName': 'Other'}}]\n"
     ]
    }
   ],
   "source": [
    "err_9 = \"The first of these scientist begin in January.\"\n",
    "err_9c = \"The first of these scientists begin in January.\"\n",
    "\n",
    "pprint(check_sentence(err_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The first of these scientist begin in January.',\n",
       " (('The', ('The',), (0.9965081810951233,)),\n",
       "  ('first', ('first',), (0.004373201169073582,)),\n",
       "  ('of', ('of',), (0.9866845011711121,)),\n",
       "  ('these', ('these',), (0.057628609240055084,)),\n",
       "  ('scientist', ('scientist',), (1.0894053437127127e-09,)),\n",
       "  ('begin', ('begin',), (0.010585297830402851,)),\n",
       "  ('in', ('in',), (0.9678221344947815,)),\n",
       "  ('January', ('January',), (0.0036275957245379686,)),\n",
       "  ('.', ('.',), (0.9448446035385132,))),\n",
       " 2.2490826196956517)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_9_res = get_word_probabilities(err_9, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_9, err_9_res, sum_log_probabilities(err_9_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The first of these scientists begin in January.',\n",
       " (('The', ('The',), (0.9947749376296997,)),\n",
       "  ('first', ('first',), (0.0005680953036062419,)),\n",
       "  ('of', ('of',), (0.91595858335495,)),\n",
       "  ('these', ('these',), (0.05670550465583801,)),\n",
       "  ('scientists', ('scientists',), (8.002268145901326e-07,)),\n",
       "  ('begin', ('begin',), (0.0017818311462178826,)),\n",
       "  ('in', ('in',), (0.8879834413528442,)),\n",
       "  ('January', ('January',), (0.001778475008904934,)),\n",
       "  ('.', ('.',), (0.9370677471160889,))),\n",
       " 4.126118188968048)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_9c_res = get_word_probabilities(err_9c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_9c, err_9c_res, sum_log_probabilities(err_9c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one of the LanguageTool suggested corrections, not so good:\n",
    "## `The first of this scientist begin in January.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The first of this scientist begin in January.',\n",
       " (('The', ('The',), (0.9427676796913147,)),\n",
       "  ('first', ('first',), (6.733025657013059e-06,)),\n",
       "  ('of', ('of',), (0.09354150295257568,)),\n",
       "  ('this', ('this',), (0.0009051816305145621,)),\n",
       "  ('scientist', ('scientist',), (2.574591064785636e-07,)),\n",
       "  ('begin', ('begin',), (0.01110108569264412,)),\n",
       "  ('in', ('in',), (0.925640344619751,)),\n",
       "  ('January', ('January',), (0.004821296781301498,)),\n",
       "  ('.', ('.',), (0.9484015703201294,))),\n",
       " -5.035691086049986)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_9clt = \"The first of this scientist begin in January.\"\n",
    "err_9clt_res = get_word_probabilities(err_9clt, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_9clt, err_9clt_res, sum_log_probabilities(err_9clt_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors with Verb Form:\n",
    "## `Brent would often became stunned by resentment.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "err_10 = \"Brent would often became stunned by resentment.\"\n",
    "err_10c = \"Brent would often become stunned by resentment.\"\n",
    "\n",
    "pprint(check_sentence(err_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error not found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Brent would often became stunned by resentment.',\n",
       " (('Brent', ('Brent',), (5.147402043803595e-05,)),\n",
       "  ('would', ('would',), (0.0011423516552895308,)),\n",
       "  ('often', ('often',), (0.0006808876059949398,)),\n",
       "  ('became', ('became',), (1.685629649728071e-05,)),\n",
       "  ('stunned', ('stunned',), (0.00016873223648872226,)),\n",
       "  ('by', ('by',), (0.32671085000038147,)),\n",
       "  ('resentment', ('resentment',), (7.069127605063841e-05,)),\n",
       "  ('.', ('.',), (0.9559879899024963,))),\n",
       " -17.498713297129644)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_10_res = get_word_probabilities(err_10, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_10, err_10_res, sum_log_probabilities(err_10_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Brent would often become stunned by resentment.',\n",
       " (('Brent', ('Brent',), (6.225280958460644e-05,)),\n",
       "  ('would', ('would',), (0.372565895318985,)),\n",
       "  ('often', ('often',), (0.022015467286109924,)),\n",
       "  ('become', ('become',), (0.028336457908153534,)),\n",
       "  ('stunned', ('stunned',), (8.714829164091498e-05,)),\n",
       "  ('by', ('by',), (0.3506300151348114,)),\n",
       "  ('resentment', ('resentment',), (5.4146814363775775e-05,)),\n",
       "  ('.', ('.',), (0.9466897249221802,))),\n",
       " -1.4844211944607055)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_10c_res = get_word_probabilities(err_10c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_10c, err_10c_res, sum_log_probabilities(err_10c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another verb form error:\n",
    "## `I having mostly been moving flat.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "err_11 = \"I having mostly been moving flat.\"\n",
    "err_11c = \"I have mostly been moving flat.\"\n",
    "\n",
    "pprint(check_sentence(err_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I having mostly been moving flat.',\n",
       " (('I', ('I',), (0.005292738322168589,)),\n",
       "  ('having', ('having',), (6.289655721047893e-05,)),\n",
       "  ('mostly', ('mostly',), (0.005705267656594515,)),\n",
       "  ('been', ('been',), (0.5970264077186584,)),\n",
       "  ('moving', ('moving',), (0.00012258144852239639,)),\n",
       "  ('flat', ('flat',), (9.995254913519602e-06,)),\n",
       "  ('.', ('.',), (0.8984378576278687,))),\n",
       " -8.98863935388643)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_11_res = get_word_probabilities(err_11, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_11, err_11_res, sum_log_probabilities(err_11_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I have mostly been moving flat.',\n",
       " (('I', ('I',), (0.11393590271472931,)),\n",
       "  ('have', ('have',), (0.41972973942756653,)),\n",
       "  ('mostly', ('mostly',), (0.005898055154830217,)),\n",
       "  ('been', ('been',), (0.9094554781913757,)),\n",
       "  ('moving', ('moving',), (0.00014484622806776315,)),\n",
       "  ('flat', ('flat',), (9.372281965625007e-06,)),\n",
       "  ('.', ('.',), (0.9684615731239319,))),\n",
       " 3.5182476527900794)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_11c_res = get_word_probabilities(err_11c, bert_tokenizer=bert_tokenizer, bert_model=bert_model)\n",
    "err_11c, err_11c_res, sum_log_probabilities(err_11c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Areas for further investigation\n",
    "* Detecting errors is difficult!\n",
    "    * Can anomaly detection be used to detect errors?\n",
    "        * e.g. anomalies in the sequence of word probabilities for a sentence\n",
    "    * Research seems to indicate that anomaly/error thresholds have to be tuned per domain.\n",
    "* Good datasets of true grammatical errors are hard to find; best to collect your own if you can.\n",
    "* Machine generated grammatical error datasets are difficult to get right; but some augmentation is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all for now!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
